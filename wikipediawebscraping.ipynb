{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiTables = pd.read_html('https://pt.wikipedia.org/wiki/Alta_Floresta_d%27Oeste')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'fundacao_emancipacao.csv' criado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# # Função para extrair a tabela 0 de uma página da Wikipedia\n",
    "# def extract_table_from_wikipedia(url):\n",
    "#     response = requests.get(url)\n",
    "#     soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#     tables = soup.find_all('table')\n",
    "#     return pd.read_html(str(tables[0]))\n",
    "\n",
    "# # Carregar o DataFrame com os nomes das cidades (vamos usar apenas os primeiros 5 registros)\n",
    "# df = pd.read_csv(\"county.csv\").head()\n",
    "\n",
    "# # Criar uma lista para armazenar todos os DataFrames\n",
    "# dfs = []\n",
    "\n",
    "# # Iterar sobre cada linha do DataFrame e extrair a tabela 0 de cada página da Wikipedia\n",
    "# for index, row in df.iterrows():\n",
    "#     city_name = row['county_name'].replace(' ', '_')\n",
    "#     url = f'https://pt.wikipedia.org/wiki/{city_name}'\n",
    "#     tables = extract_table_from_wikipedia(url)\n",
    "#     if tables:\n",
    "#         dfs.append(tables[0])\n",
    "\n",
    "# # Concatenar todos os DataFrames em um único DataFrame\n",
    "# result_df = pd.concat(dfs)\n",
    "\n",
    "# # Salvar o DataFrame em um arquivo CSV usando o Pandas\n",
    "# result_df.to_csv(\"tabelas_wikipedia.csv\", index=False)\n",
    "\n",
    "# print(\"Arquivo 'tabelas_wikipedia.csv' criado com sucesso!\")\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Função para extrair a tabela 0 de uma página da Wikipedia\n",
    "def extract_table_from_wikipedia(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    tables = soup.find_all('table')\n",
    "    for table in tables:\n",
    "        headers = table.find_all('th')\n",
    "        for header in headers:\n",
    "            if 'história' in header.text.lower():\n",
    "                df = pd.read_html(str(table))[0]  # Apenas o primeiro DataFrame\n",
    "                # Filtrar as linhas que contêm \"Fundação\" ou \"Emancipação\" na parte de história\n",
    "                relevant_rows = df[df.iloc[:,0].str.contains('Fundação|Emancipação', case=False, na=False)]\n",
    "                return relevant_rows\n",
    "    return None\n",
    "\n",
    "# Carregar o DataFrame com os nomes das cidades\n",
    "df = pd.read_csv(\"county.csv\")\n",
    "\n",
    "# Criar listas para armazenar os nomes das cidades, os anos de fundação e os anos de emancipação\n",
    "cities = []\n",
    "foundation_years = []\n",
    "emancipation_years = []\n",
    "\n",
    "# Iterar sobre cada linha do DataFrame e extrair a tabela 0 de cada página da Wikipedia\n",
    "for index, row in df.iterrows():\n",
    "    city_name = row['county_name'].replace(' ', '_')\n",
    "    url = f'https://pt.wikipedia.org/wiki/{city_name}'\n",
    "    table = extract_table_from_wikipedia(url)\n",
    "    if table is not None:\n",
    "        city_foundation = None\n",
    "        city_emancipation = None\n",
    "        for _, row in table.iterrows():\n",
    "            if 'Fundação' in row[0]:\n",
    "                city_foundation = row[1]\n",
    "            elif 'Emancipação' in row[0]:\n",
    "                city_emancipation = row[1]\n",
    "        cities.append(city_name)\n",
    "        foundation_years.append(city_foundation)\n",
    "        emancipation_years.append(city_emancipation)\n",
    "\n",
    "# Criar um DataFrame com os dados coletados\n",
    "result_df = pd.DataFrame({'Cidade': cities, 'Ano de Fundação': foundation_years, 'Ano de Emancipação': emancipation_years})\n",
    "\n",
    "# Salvar o DataFrame em um arquivo CSV usando o Pandas\n",
    "result_df.to_csv(\"fundacao_emancipacao.csv\", index=False)\n",
    "\n",
    "print(\"Arquivo 'fundacao_emancipacao.csv' criado com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for x in range(1,10):\n",
    "    print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
